# Pending Tasks - Speed Optimization System

**Status**: ~95% Complete | **Blocker**: Prisma Client Types

---

## üö® CRITICAL BLOCKER (Must Fix First)

### ‚ùå Prisma Client Type Generation
**Issue**: TypeScript compilation errors - Prisma types don't include `ProjectContext` model

**Current Errors** (10 total):
```
packages/backend/src/modules/projects/services/project-context.service.ts:
- Line 40: Property 'context' does not exist in type 'ProjectInclude'
- Line 47: Property 'schema' does not exist on type 'Project'
- Line 52: Property 'context' does not exist on type 'Project'
- Line 77: Property 'projectContext' does not exist on type 'PrismaService'
- Line 115: Property 'projectContext' does not exist on type 'PrismaService'
- Line 137: Property 'projectContext' does not exist on type 'PrismaService'

packages/backend/src/modules/ai/services/chatbot.service.ts:
- Line 814: Property 'userFeedback' does not exist on type 'PrismaService'
```

**Root Cause**: Windows file lock preventing `npx prisma generate` from completing
```
EPERM: operation not permitted, rename
'node_modules\.pnpm\@prisma+client@6.18.0...\query_engine-windows.dll.node.tmp9576'
```

**Solution**:
```bash
# 1. Close VS Code completely
# 2. Stop all backend processes
# 3. Open new terminal and run:
cd d:/Work/ai-service-platform/packages/backend
npx prisma generate

# 4. Restart VS Code
# 5. Verify errors cleared
```

**Verification**:
- Check `node_modules/.prisma/client/index.d.ts` contains `ProjectContext` type
- TypeScript errors should disappear
- Backend should compile without errors

---

## ‚ö†Ô∏è HIGH PRIORITY (After Prisma Fixed)

### 1Ô∏è‚É£ Backend Compilation & Testing
**What**: Verify backend compiles and all API endpoints work

**Steps**:
```bash
# Start backend
cd d:/Work/ai-service-platform/packages/backend
npm run start:dev

# Should see:
# ‚úÖ Application is running on: http://localhost:3001
# ‚úÖ No compilation errors
```

**Test API Endpoints**:
```bash
# 1. Health check
curl http://localhost:3001/api/v1/health

# 2. Get context template
curl -H "Authorization: Bearer <token>" \
  http://localhost:3001/api/v1/projects/<project-id>/context/template

# 3. Generate AI context
curl -X POST -H "Authorization: Bearer <token>" \
  http://localhost:3001/api/v1/projects/<project-id>/context/generate?force=true

# 4. Get initial suggestions
curl -H "Authorization: Bearer <token>" \
  http://localhost:3001/api/v1/projects/<project-id>/context/suggestions/initial
```

**Expected Results**:
- All endpoints return 200 OK
- Template returns JSON structure
- Generate context returns AI-analyzed context
- Suggestions return 4 prompt strings

---

### 2Ô∏è‚É£ Ollama Model Verification
**What**: Ensure qwen2.5:0.5b model is pulled and working

**Steps**:
```bash
# Start Ollama container
docker-compose -f docker-compose.ollama.yml up -d

# Pull 0.5b model
docker exec ai-service-ollama ollama pull qwen2.5:0.5b

# Verify model loaded
docker exec ai-service-ollama ollama list

# Should see:
# qwen2.5:0.5b    490MB    ...
# qwen2.5:1.5b    1.5GB    ...
```

**Test Speed**:
```bash
# Time a simple query
time docker exec ai-service-ollama ollama run qwen2.5:0.5b "What is 2+2?"

# Expected: ~0.5-1 second response time
```

---

## üìù MEDIUM PRIORITY (API Integration)

### 3Ô∏è‚É£ Frontend API Hooks - Schema Page (Context Tab)

**File**: `packages/frontend/app/dashboard/schema/page.tsx`

**Add Imports**:
```tsx
import { useMutation, useQuery } from '@tanstack/react-query';
import { useToast } from '@/hooks/use-toast';
```

**Add API Calls** (after line 72):
```tsx
const { toast } = useToast();

// Fetch existing context
const { data: projectContext, refetch: refetchContext } = useQuery({
  queryKey: ['project-context', selectedProjectId],
  queryFn: async () => {
    const response = await api.get(`/projects/${selectedProjectId}/context`);
    return response.data;
  },
  enabled: !!selectedProjectId,
});

// Generate AI context
const generateContextMutation = useMutation({
  mutationFn: async (projectId: string) => {
    const response = await api.post(`/projects/${projectId}/context/generate?force=true`);
    return response.data;
  },
  onSuccess: () => {
    toast({
      title: 'Success',
      description: 'AI context generated successfully!',
    });
    refetchContext();
  },
  onError: (error: any) => {
    toast({
      variant: 'destructive',
      title: 'Error',
      description: error.response?.data?.message || 'Failed to generate context',
    });
  },
});

// Download template
const handleDownloadTemplate = async () => {
  try {
    const response = await api.get(`/projects/${selectedProjectId}/context/template`);
    const blob = new Blob([JSON.stringify(response.data, null, 2)], { type: 'application/json' });
    const url = URL.createObjectURL(blob);
    const a = document.createElement('a');
    a.href = url;
    a.download = 'project-context-template.json';
    a.click();
    URL.revokeObjectURL(url);
  } catch (error: any) {
    toast({
      variant: 'destructive',
      title: 'Error',
      description: error.response?.data?.message || 'Failed to download template',
    });
  }
};

// Upload manual context
const uploadContextMutation = useMutation({
  mutationFn: async ({ projectId, json }: { projectId: string; json: any }) => {
    const response = await api.put(`/projects/${projectId}/context/manual`, json);
    return response.data;
  },
  onSuccess: () => {
    toast({
      title: 'Success',
      description: 'Manual context uploaded successfully!',
    });
    refetchContext();
  },
  onError: (error: any) => {
    toast({
      variant: 'destructive',
      title: 'Error',
      description: error.response?.data?.message || 'Failed to upload context',
    });
  },
});

// Handle file upload
const handleContextFileUpload = async (event: React.ChangeEvent<HTMLInputElement>) => {
  const file = event.target.files?.[0];
  if (!file) return;

  try {
    const text = await file.text();
    const json = JSON.parse(text);
    
    if (!selectedProjectId) {
      toast({
        variant: 'destructive',
        title: 'Error',
        description: 'Please select a project first',
      });
      return;
    }

    uploadContextMutation.mutate({ projectId: selectedProjectId, json });
  } catch (error) {
    toast({
      variant: 'destructive',
      title: 'Error',
      description: 'Invalid JSON file',
    });
  }
};
```

**Update UI Buttons** (replace placeholders):
```tsx
{/* Line ~715: Generate AI Context button */}
<Button 
  onClick={() => selectedProjectId && generateContextMutation.mutate(selectedProjectId)}
  disabled={!selectedProjectId || generateContextMutation.isPending}
>
  <Sparkles className="mr-2 h-4 w-4" />
  {generateContextMutation.isPending ? 'Generating...' : 'Generate AI Context'}
</Button>

{/* Line ~738: Download Template button */}
<Button 
  variant="outline" 
  onClick={handleDownloadTemplate}
  disabled={!selectedProjectId}
>
  <Download className="mr-2 h-4 w-4" />
  Download Template
</Button>

{/* Line ~746: Upload file input */}
<Input
  id="context-file"
  type="file"
  accept=".json"
  onChange={handleContextFileUpload}
  disabled={uploadContextMutation.isPending}
/>
```

**Display Generated Context** (add after upload card):
```tsx
{/* Show generated context if exists */}
{projectContext && (
  <Card className="mt-4">
    <CardHeader>
      <CardTitle>Current Context</CardTitle>
      <CardDescription>
        AI-generated and manual context merged
      </CardDescription>
    </CardHeader>
    <CardContent>
      <div className="space-y-2">
        <div>
          <p className="text-sm font-medium">Context Summary:</p>
          <p className="text-sm text-muted-foreground">{projectContext.contextSummary}</p>
        </div>
        {projectContext.initialPrompts && projectContext.initialPrompts.length > 0 && (
          <div>
            <p className="text-sm font-medium">Initial Prompts:</p>
            <ul className="list-disc list-inside text-sm text-muted-foreground">
              {projectContext.initialPrompts.map((prompt: string, idx: number) => (
                <li key={idx}>{prompt}</li>
              ))}
            </ul>
          </div>
        )}
      </div>
    </CardContent>
  </Card>
)}
```

---

### 4Ô∏è‚É£ Frontend API Hooks - Chat Page (Suggestions)

**File**: `packages/frontend/app/dashboard/chat/page.tsx`

**Replace Hardcoded Suggestions** (line ~72):
```tsx
// OLD:
const [suggestionPrompts, setSuggestionPrompts] = useState<string[]>([
  "Show me all users in the database",
  "What tables are related to orders?",
  "Analyze sales data for last month",
  "Find customers with no orders"
]);

// NEW:
const { data: suggestionPrompts = [], isLoading: loadingSuggestions } = useQuery({
  queryKey: ['initial-suggestions', selectedProjectId],
  queryFn: async () => {
    if (!selectedProjectId) return [];
    const response = await api.get(`/projects/${selectedProjectId}/context/suggestions/initial`);
    return response.data.suggestions || [];
  },
  enabled: !!selectedProjectId,
});
```

**Update Empty State** (line ~488):
```tsx
{messages.length === 0 && (
  <div className="flex flex-col items-center justify-center h-full p-8">
    <MessageSquare className="h-12 w-12 text-muted-foreground mb-4" />
    <h3 className="text-lg font-semibold mb-2">Start a Conversation</h3>
    <p className="text-sm text-muted-foreground mb-6 text-center max-w-md">
      Ask questions about your database schema, request SQL queries, or explore your data.
    </p>
    
    {/* Suggestion prompts grid */}
    {loadingSuggestions ? (
      <p className="text-sm text-muted-foreground">Loading suggestions...</p>
    ) : suggestionPrompts.length > 0 ? (
      <div className="grid grid-cols-1 md:grid-cols-2 gap-3 w-full max-w-2xl">
        {suggestionPrompts.map((prompt, idx) => (
          <Button
            key={idx}
            variant="outline"
            className="h-auto py-3 px-4 text-left justify-start whitespace-normal"
            onClick={() => setInput(prompt)}
          >
            <Sparkles className="mr-2 h-4 w-4 flex-shrink-0" />
            <span className="text-sm">{prompt}</span>
          </Button>
        ))}
      </div>
    ) : selectedProjectId ? (
      <p className="text-sm text-muted-foreground">
        Upload a schema first to see suggestions
      </p>
    ) : (
      <p className="text-sm text-muted-foreground">
        Select a project to start chatting
      </p>
    )}
  </div>
)}
```

**Add Follow-up Suggestions** (after message send):
```tsx
// Add after line ~150 (sendMessage function)
const getFollowUpSuggestions = async (query: string, response: string) => {
  try {
    const result = await api.post(
      `/projects/${selectedProjectId}/context/suggestions/followup`,
      { lastQuery: query, lastResponse: response }
    );
    // Optionally show these as quick reply buttons below AI response
    return result.data.suggestions || [];
  } catch (error) {
    console.error('Failed to get follow-up suggestions:', error);
    return [];
  }
};
```

---

## üß™ LOW PRIORITY (Testing & Verification)

### 5Ô∏è‚É£ End-to-End Testing Flow

**Test Scenario**: Complete user journey from schema upload to fast chat responses

**Steps**:
1. ‚úÖ Login to dashboard
2. ‚úÖ Create new project
3. ‚úÖ Upload database schema (Schema tab)
4. ‚úÖ Navigate to Context tab
5. ‚úÖ Click "Generate AI Context" ‚Üí Verify context appears
6. ‚úÖ Click "Download Template" ‚Üí Verify JSON downloads
7. ‚úÖ Edit JSON with custom context ‚Üí Upload ‚Üí Verify merged
8. ‚úÖ Navigate to Chat tab
9. ‚úÖ Verify 4 AI-generated suggestions appear
10. ‚úÖ Click suggestion ‚Üí Verify input populated
11. ‚úÖ Send message ‚Üí Verify response time <1s
12. ‚úÖ Check response accuracy and format
13. ‚úÖ Test 5-10 queries, verify average response time

**Performance Benchmarks**:
- **Before**: 5-7 seconds per query (qwen2.5:1.5b + verbose CoT)
- **Target After**: <1 second simple queries, <2 seconds complex queries
- **Improvement**: 7x faster (86% reduction in response time)

---

### 6Ô∏è‚É£ Docker Compose Verification

**Ensure All Services Running**:
```bash
# Start all services
docker-compose -f docker-compose.yml -f docker-compose.ollama.yml up -d

# Check status
docker-compose ps

# Should see:
# ai-service-postgres    running
# ai-service-mongodb     running
# ai-service-redis       running
# ai-service-weaviate    running
# ai-service-ollama      running
# ai-service-backend     running (after Prisma fixed)
# ai-service-frontend    running
```

**Check Logs**:
```bash
# Backend logs (verify no errors)
docker-compose logs -f backend

# Ollama logs (verify model loaded)
docker-compose -f docker-compose.ollama.yml logs -f ollama
```

---

## üìä Summary Checklist

### Critical Path (Must Complete in Order):
- [ ] **1. Fix Prisma Client Generation** (Blocker - closes VS Code, regenerate)
- [ ] **2. Verify Backend Compiles** (npm run start:dev succeeds)
- [ ] **3. Test Backend API Endpoints** (6 endpoints working)
- [ ] **4. Verify Ollama Model** (qwen2.5:0.5b pulled and fast)
- [ ] **5. Connect Frontend APIs** (Schema page + Chat page hooks)
- [ ] **6. End-to-End Testing** (Full user journey works)

### Feature Completion Status:
‚úÖ **Database Schema**: ProjectContext model created and migrated  
‚úÖ **Backend Services**: ProjectContextService (9 methods) completed  
‚úÖ **Backend Controllers**: 6 API endpoints implemented  
‚úÖ **Module Wiring**: ProjectsModule, AiModule updated  
‚úÖ **Prompt Optimization**: 64% token reduction (2400 ‚Üí 870 tokens)  
‚úÖ **Model Switch**: qwen2.5:0.5b configured  
‚úÖ **Frontend UI**: Context tab + suggestion prompts created  
‚ö†Ô∏è **Backend Compilation**: Blocked by Prisma types (CRITICAL)  
‚ùå **Frontend API Integration**: Not started (needs backend working)  
‚ùå **E2E Testing**: Not started  

### Time Estimates:
- ‚è±Ô∏è **Prisma Fix**: 5 minutes (close/regenerate/reopen)
- ‚è±Ô∏è **Backend Testing**: 10 minutes (start server + test endpoints)
- ‚è±Ô∏è **Ollama Verification**: 5 minutes (pull model + test speed)
- ‚è±Ô∏è **Frontend API Hooks**: 30 minutes (implement all mutations/queries)
- ‚è±Ô∏è **E2E Testing**: 15 minutes (full user flow)
- **Total Remaining**: ~65 minutes to 100% completion

---

## üéØ Expected Final Outcome

### Performance Metrics:
- **Response Time**: <1s for simple queries (90%+ queries)
- **Complex Queries**: <2s (with RAG + schema analysis)
- **Improvement**: 7x faster than previous 5-7s average
- **Token Usage**: 64% reduction in prompt size
- **Model Size**: 500MB vs 1.5GB (3x smaller)

### Feature Set:
‚úÖ **AI-Generated Context**: Automatic schema analysis and context creation  
‚úÖ **Manual Context Upload**: Custom JSON templates for project-specific knowledge  
‚úÖ **Merged Context**: Combines AI + manual for optimal accuracy  
‚úÖ **Initial Suggestions**: 4 smart prompts based on schema  
‚úÖ **Follow-up Suggestions**: Context-aware next questions  
‚úÖ **Fast Model**: qwen2.5:0.5b for sub-second responses  
‚úÖ **Optimized Prompts**: Concise, focused, no verbose CoT  
‚úÖ **Template System**: Downloadable JSON structure for easy customization

### User Experience:
1. Upload schema once
2. AI generates context automatically
3. Optionally customize with manual JSON
4. See 4 suggested questions immediately
5. Get instant (<1s) responses
6. Enjoy accurate SQL generation and insights
7. No more waiting 5-7 seconds per query

---

**NEXT IMMEDIATE ACTION**: Close VS Code ‚Üí Run `npx prisma generate` ‚Üí Reopen VS Code ‚Üí Continue with testing
